{
  "agent": "tensorforce",
  "states": {
    "type": "float",
    "shape": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    "min_value": [0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0, 0.0, 0.0, -24.0],
    "max_value": [360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0, 450.0, 360.0, 24.0]
  },
  "actions": {
    "type": "int",
    "shape": [0.0],
    "num_values": 3
  },
  "memory": 10000,
  "update": {
    "unit": "timesteps",
    "batch_size": 64
  },
  "optimizer": {
    "type": "adam",
    "learning_rate": 1e-3
  },
  "policy": {
    "network": "auto"
  },
  "objective": "policy_gradient",
  "reward_estimation": {
    "horizon": 20
  }
}